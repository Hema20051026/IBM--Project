# -*- coding: utf-8 -*-
"""Health Al.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KKBEhtTRUvd0kiCOy0Rn7aZfQ4IOuU05
"""

"! pip install transformers torch gradio. -q".

"!pip install transformers torch gradio -q"

EduTutorAl.ipynb
Changes will not be saved
File Edit View Insert Runtime Tools Help
Q Commands
!!!
+ Code + Text
▷ Run all▾
Copy to Drive
1 # Educational AI Application using IBM Granite Model
2 # Run this in Google Colab
3 !pip install transformers torch gradio -q
@
[]
<>
O
1 import gradio as gr
2 import torch
3 from transformers import AutoTokenizer, AutoModelForCausalLM
5 # Load model and tokenizer
6 model_name = "ibm-granite/granite-3.2-2b-instruct"
7 tokenizer - AutoTokenizer.from_pretrained (model_name)
8 model AutoModelForCausalLM. from pretrained(
9
10
11
12)
13
model name,
torch_dtype-torch. Float16 if torch.cuda.is available() else torch.float32,
device_map="auto" if torch.cuda.is_available() else None
14 if tokenizer.pad token is None:
15
16
tokenizer.pad_token = tokenizer.eos_token
17 def generate response(prompt, max_length=512):
18
19
28
21
22
23
25
26
27
28
29
30
inputs tokenizer(prompt, return_tensors="pt", truncation-True, max_length=512)
if torch.cuda.is_available();
inputs (k: v.to(model.device) for k, v in inputs.items())
with torch.no_grad():
outputs model.generate(
**inputs,
max_length=max_length,
temperature-0.7,
do_sample=True,
pad_token_id-tokenizer.eos_token_id.
CO
EduTutorAl.ipynb
Cannot save changes
File Edit View Insert Runtime Tools Help
Q Commands
+ Code + Text ▷ Run all▾ Copy to Drive
!!! O
31
32
response
33
response
tokenizer.decode(outputs[0], skip_special_tokens-True)
response.replace(prompt, "").strip()
34
return response
35
<>
36 def concept_explanation (concept):
37
prompt
f"Explain the concept of [concept) in detail with examples:"
38
return generate response(prompt, max_length=800)
39
11
•
Share
✦ Gemini
V
Connect 14 ▾
↑
↓
+
G
Share
Gemini
V
40 def quiz generator(concept):
prompt f"Generate 5 quiz questions about (concept) with different question types (multiple choice, true/false, short answer). At the end, provide all the answers in a separate ANSWERS secti
42 return generate response(prompt, max length-1000)
43
44 # Create Gradio interface.
45 with gr.Blocks() as app:
gr.Markdown("# Educational AI Assistant")
46
47
with gr.Tabs():
49
51
52
55
57
with gr.TabItem("Concept Explanation"):
concept_input gr.Textbox (label="Enter a concept", placeholder="e.g., machine learning")
explain_btn = gr.Button("Explain")
explanation output = gr.Textbox(label="Explanation", lines-10)
explain_btn.click(concept_explanation, inputs-concept_input, outputs-explanation_output)
with gr.TabItem("Quiz Generator"):
quiz_input = gr.Textbox(label="Enter a topic", placeholder="e.g., physics")
quiz btn gr.Button("Generate Quiz")
quiz_output = gr.Textbox (label="Quiz Questions", lines-15)
quiz_btn.click(quiz generator, inputs-quiz_input, outputs-quiz_output)
63 app.launch(share=True)
You can find the code here in this link: Edu Tutor AI Code

import gradio as gr
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load model and tokenizer
model_name = "ibm-granite/granite-3.2-2b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto" if torch.cuda.is_available() else None
)

if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def generate_response(prompt, max_length=512):
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=512)
    if torch.cuda.is_available():
        inputs = {k: v.to(model.device) for k, v in inputs.items()}
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response.replace(prompt, "").strip()
    return response

def concept_explanation(concept):
    prompt = f"Explain the concept of {concept} in detail with examples:"
    return generate_response(prompt, max_length=800)

def quiz_generator(concept):
    prompt = f"Generate 5 quiz questions about {concept} with different question types (multiple choice, true/false, short answer). At the end, provide all the answers in a separate ANSWERS section."
    return generate_response(prompt, max_length=1000)

# Create Gradio interface.
with gr.Blocks() as app:
    gr.Markdown("# Educational AI Assistant")
    with gr.Tabs():
        with gr.TabItem("Concept Explanation"):
            concept_input = gr.Textbox(label="Enter a concept", placeholder="e.g., machine learning")
            explain_btn = gr.Button("Explain")
            explanation_output = gr.Textbox(label="Explanation", lines=10)
            explain_btn.click(concept_explanation, inputs=concept_input, outputs=explanation_output)
        with gr.TabItem("Quiz Generator"):
            quiz_input = gr.Textbox(label="Enter a topic", placeholder="e.g., physics")
            quiz_btn = gr.Button("Generate Quiz")
            quiz_output = gr.Textbox(label="Quiz Questions", lines=15)
            quiz_btn.click(quiz_generator, inputs=quiz_input, outputs=quiz_output)

app.launch(share=True)



